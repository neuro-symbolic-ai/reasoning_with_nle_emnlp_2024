{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOOaOgmDEkKd"
   },
   "source": [
    "# Inductive Logic Programming with Large Language Models (LLMs)\n",
    "\n",
    "Given a background knowledge (BK), a set of logical clauses that represent prior knowledge about the domain a set of positive examples (E+), a set of ground facts (instances) which the learned theory should entail (i.e., these are examples that should be true according to the theory), a set of negative examples (E−), a set of ground facts which the learned theory should not entail (i.e., these are examples that should be false according to the theory). Find a hypothesis H (a set of logical clauses) such that:\n",
    "\n",
    "- *Completeness:* For every example $e \\in E^+, H \\cup BK \\models e$, meaning the hypothesis H, together with the background knowledge BK, should entail all positive examples.\n",
    "- *Consistency:* For every example $e \\in E^−, H \\cup BK \\not\\models e$, meaning the hypothesis H, together with the background knowledge BK, should not entail any negative examples.\n",
    "\n",
    "\\begin{equation}\n",
    "\\forall e \\in E^+, H \\cup BK \\models e\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\forall e \\in E^−, H \\cup BK \\not\\models e\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"figures/hybrid_ilp.png\">\n",
    "\n",
    "## Evaluation\n",
    "\n",
    " Checking for the completeness and consistency conditions:\n",
    " \n",
    " - *True Positives (TP):* The number of positive examples correctly entailed by the hypothesis.\n",
    " - *False Positives (FP):* The number of negative examples incorrectly entailed by the hypothesis.\n",
    " - *False Negatives (FN):* The number of positive examples not entailed by the hypothesis.\n",
    " - *True Negatives (TN):* The number of negative examples correctly not entailed by the hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalising the ancestor relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1578,
     "status": "ok",
     "timestamp": 1730472630514,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "dlO68LQU95_5",
    "outputId": "47318741-09eb-4f8d-cc1e-1ac1a9968f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background Knowledge\n",
      "\n",
      "parent(john, mary).\n",
      "parent(mary, susan).\n",
      "parent(john, michael).\n",
      "parent(michael, robert).\n",
      "\n",
      "Positive and Negative Examples\n",
      "\n",
      "pos(ancestor(john, susan)).\n",
      "pos(ancestor(john, robert)).\n",
      "pos(ancestor(mary, susan)).\n",
      "neg(ancestor(mary, robert)).\n",
      "neg(ancestor(michael, susan)).\n",
      "\n",
      "Generated Theory\n",
      "\n",
      "prolog\n",
      "ancestor(X, Y) :- parent(X, Y).\n",
      "ancestor(X, Y) :- parent(X, Z), ancestor(Z, Y).\n",
      "\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Accuracy 1.0\n",
      "F1 score 1.0\n",
      "{'false_positives': [], 'false_negatives': []}\n"
     ]
    }
   ],
   "source": [
    "#Import the relevant libraries\n",
    "from critique.inductive_logic_programming import ILPSolver\n",
    "from generation.generative_model import GPT\n",
    "import yaml\n",
    "\n",
    "\n",
    "data_name = 'example_1'\n",
    "\n",
    "#Instantiate the generative model. We use GPT-4o-mini\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    api_key = config.get('gpt-4o-mini', {}).get('api_key')\n",
    "\n",
    "llm = GPT('gpt-4o-mini', api_key)\n",
    "\n",
    "\n",
    "# Instantiate the ILPSolver. The solver uses Prolog to evaluate the theory generated by the LLM \n",
    "ilp_solver = ILPSolver(\n",
    "    generative_model=llm,\n",
    "    theory_name = data_name\n",
    ")\n",
    "\n",
    "result = ilp_solver.critique()\n",
    "\n",
    "print(\"Background Knowledge\\n\")\n",
    "background_data = open(f\"./formalisation/prolog/{data_name}/bk.pl\", 'r')\n",
    "print(background_data.read())\n",
    "\n",
    "print(\"\\nPositive and Negative Examples\\n\")\n",
    "examples = open(f\"./formalisation/prolog/{data_name}/exs.pl\", 'r')\n",
    "print(examples.read())\n",
    "\n",
    "print(\"\\nGenerated Theory\\n\")\n",
    "print(result[\"generated_theory\"])\n",
    "\n",
    "print(\"\\nEvaluation\\n\")\n",
    "print(\"Accuracy\", result['accuracy'])\n",
    "print(\"F1 score\", result['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalising the granparent relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3391,
     "status": "ok",
     "timestamp": 1730473106847,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "BIVihpPsBUan",
    "outputId": "a4b6a197-4f9c-429e-f682-9e2f18b0a7c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background Knowledge\n",
      "\n",
      "mother(ann,amy).\n",
      "mother(ann,andy).\n",
      "mother(amy,amelia).\n",
      "mother(linda,gavin).\n",
      "father(steve,amy).\n",
      "father(steve,andy).\n",
      "father(gavin,amelia).\n",
      "father(andy,spongebob).\n",
      "\n",
      "Positive and Negative Examples\n",
      "\n",
      "pos(grandparent(ann,amelia)).\n",
      "pos(grandparent(steve,amelia)).\n",
      "pos(grandparent(ann,spongebob)).\n",
      "pos(grandparent(steve,spongebob)).\n",
      "pos(grandparent(linda,amelia)).\n",
      "neg(grandparent(amy,amelia)).\n",
      "\n",
      "Generated Theory\n",
      "\n",
      "prolog\n",
      "grandparent(X, Y) :- mother(X, Z), mother(Z, Y).\n",
      "grandparent(X, Y) :- father(X, Z), mother(Z, Y).\n",
      "grandparent(X, Y) :- mother(X, Z), father(Z, Y).\n",
      "grandparent(X, Y) :- father(X, Z), father(Z, Y).\n",
      "\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Accuracy 1.0\n",
      "F1 score 1.0\n",
      "{'false_positives': [], 'false_negatives': []}\n"
     ]
    }
   ],
   "source": [
    "# Example from https://github.com/logic-and-learning-lab/Popper/tree/main/examples/kinship-pi\n",
    "from critique.inductive_logic_programming import ILPSolver\n",
    "from generation.generative_model import GPT\n",
    "import yaml\n",
    "\n",
    "data_name = 'example_2'\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    api_key = config.get('gpt-4o-mini', {}).get('api_key')\n",
    "llm = GPT('gpt-4o-mini', api_key)\n",
    "\n",
    "\n",
    "ilp_solver = ILPSolver(\n",
    "    generative_model=llm,\n",
    "    theory_name = data_name\n",
    ")\n",
    "\n",
    "result = ilp_solver.critique()\n",
    "\n",
    "print(\"Background Knowledge\\n\")\n",
    "background_data = open(f\"./formalisation/prolog/{data_name}/bk.pl\", 'r')\n",
    "print(background_data.read())\n",
    "\n",
    "print(\"\\nPositive and Negative Examples\\n\")\n",
    "examples = open(f\"./formalisation/prolog/{data_name}/exs.pl\", 'r')\n",
    "print(examples.read())\n",
    "\n",
    "print(\"\\nGenerated Theory\\n\")\n",
    "print(result[\"generated_theory\"])\n",
    "\n",
    "print(\"\\nEvaluation\\n\")\n",
    "print(\"Accuracy\", result['accuracy'])\n",
    "print(\"F1 score\", result['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of elements in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1422,
     "status": "ok",
     "timestamp": 1730473117276,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "2C4FgjVBLHdS",
    "outputId": "a6a98301-4386-4928-ab3c-f80e58e7f5a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background Knowledge\n",
      "\n",
      "tail([_|T],T).\n",
      "head([H|_],H).\n",
      "empty([]).\n",
      "zero(0).\n",
      "one(1).\n",
      "succ(0,1).\n",
      "succ(1,2).\n",
      "succ(2,3).\n",
      "succ(3,4).\n",
      "succ(4,5).\n",
      "succ(5,6).\n",
      "succ(6,7).\n",
      "succ(7,8).\n",
      "succ(8,9).\n",
      "succ(9,10).\n",
      "\n",
      "Positive and Negative Examples\n",
      "\n",
      "pos(f([1],1)).\n",
      "pos(f([2,2],2)).\n",
      "pos(f([3,3,1],3)).\n",
      "pos(f([4],1)).\n",
      "pos(f([4,3],2)).\n",
      "pos(f([4,3,2,2,3,5,2,7],8)).\n",
      "neg(f([3,3,1],2)).\n",
      "neg(f([4],0)).\n",
      "neg(f([4],2)).\n",
      "\n",
      "Generated Theory\n",
      "\n",
      "prolog\n",
      "f(L, H) :- head(L, H).\n",
      "f(L, H) :- tail(L, T), f(T, H1), H is H1 + 1.\n",
      "\n",
      "\n",
      "Evaluation\n",
      "\n",
      "Accuracy 0.7777777777777778\n",
      "F1 score 0.8\n",
      "{'false_positives': [], 'false_negatives': ['f([4],1)', 'f([4,3],2)']}\n"
     ]
    }
   ],
   "source": [
    "# Example from https://github.com/logic-and-learning-lab/Popper/tree/main/examples/synthesis-length\n",
    "from critique.inductive_logic_programming import ILPSolver\n",
    "from generation.generative_model import GPT\n",
    "import yaml\n",
    "\n",
    "\n",
    "data_name = 'example_len'\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    api_key = config.get('gpt-4o-mini', {}).get('api_key')\n",
    "llm = GPT('gpt-4o-mini', api_key)\n",
    "\n",
    "\n",
    "ilp_solver = ILPSolver(\n",
    "    generative_model=llm,\n",
    "    theory_name = data_name\n",
    ")\n",
    "\n",
    "result = ilp_solver.critique()\n",
    "\n",
    "print(\"Background Knowledge\\n\")\n",
    "background_data = open(f\"./formalisation/prolog/{data_name}/bk.pl\", 'r')\n",
    "print(background_data.read())\n",
    "\n",
    "print(\"\\nPositive and Negative Examples\\n\")\n",
    "examples = open(f\"./formalisation/prolog/{data_name}/exs.pl\", 'r')\n",
    "print(examples.read())\n",
    "\n",
    "print(\"\\nGenerated Theory\\n\")\n",
    "print(result[\"generated_theory\"])\n",
    "\n",
    "print(\"\\nEvaluation\\n\")\n",
    "print(\"Accuracy\", result['accuracy'])\n",
    "print(\"F1 score\", result['f1'])\n",
    "print(result['wrong_examples'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPJsBXtu7vzsQXmC7tVSZRC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
