{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV77S90jvsO0"
   },
   "source": [
    "## Inference to the Best Explanation (IBE) in Large Language Models (LLMs)\n",
    "\n",
    "IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features. It operates on top of natural language explanations generated by Large Language Models using a combination of hard and soft critique models as a proxy to assess consistency, parsimony, coherence, and uncertainty.\n",
    "\n",
    "<img src=\"figures/ibe.png\" height=\"400\" class=\"center\">\n",
    "\n",
    "## IBE Evaluation Criteria\n",
    "\n",
    "- *Consistency (Hard Critique).* Verify whether the explanation is logically valid. Given a hypothesis, composed of a premise pi, a conclusion ci, and an explanation consisting of a set of statements E =s1,...,si, we define E to be logically consistent if pi ∪ E ⊨ ci. Specifically, an explanation is logically consistent if it is possible to build a deductive proof linking premise and conclusion.\n",
    "\n",
    "- *Parsimony (Soft Critique).* The parsimony principle, also known as Ockham’s razor, favors the selection of the simplest explanation consisting of the fewest elements and assumptions. Adopt two metrics as a proxy of parsimony, namely proof depth, and concept drift.  Concept drift, denoted as Drift, is defined as the\n",
    "number of additional concepts and entities, outside the ones appearing in the hypothesis (i.e., premise and conclusion), that are introduced by the LLM to support the entailment. \n",
    "\n",
    "\\begin{equation}\n",
    "Drift(h) = |Noun_{E} - (Noun_{p} \\cup Noun_{c})\n",
    "\\end{equation}\n",
    "\n",
    "- *Coherence (Soft Critique).* Attempts to measure the logical relations within individual explanatory statements and implications. An explanation can be formally consistent on the surface while still including implausible or ungrounded intermediate assumptions. Coherence evaluate the quality of each intermediate If-Then implication by measuring the entailment strength between the If and Then clauses. To this end, we employ a fine-tuned natural language inference (NLI) model. Let S\n",
    "be a set of explanation steps, where each step s consists of an If-Then statement, s = (Ifs,Thens). For a given step si, let ES(si) denote the entailment score obtained via the NLI model between Ifs and Thens clauses. The step-wise entailment score SWE(S) is then calculated as the averaged sum of the entailment scores across all explanation steps |S|.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{SWE}(S) = \\frac{1}{|S|}\\sum_{i=1}^{|S|} \\text{ES}(s_i)\n",
    "\\end{equation}\n",
    "\n",
    "- *Uncertainty (Soft Critique).* Finally, we consider the linguistic certainty expressed in the generated explanation as a proxy for plausibility. Hedging words such as probably, might be, could be, etc typically signal ambiguity and are often used when the truth condition of a statement is unknown or improbable. Pei and Jurgens (2021) found that the strength of scientific claims in research papers is strongly correlated with the use of direct language. In contrast, they found that the use of hedging language suggested that the veracity of the claim was weaker or highly contextualized. To measure the linguistic certainty we use a fine-tuned sentence-level RoBERTa model.\n",
    "\n",
    "## Results\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/ibe_results.png\" height=\"265\">\n",
    "<img src=\"figures/ibe_results_1.png\" height=\"265\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try with GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Explanations for the hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26061,
     "status": "ok",
     "timestamp": 1731635322587,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "fHzztvy0vsO8",
    "outputId": "c12b0866-a641-43d4-eb35-5ee3d1a5a238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanation 1:\n",
      "\n",
      "Hypothesis I blew into the baloon.\n",
      "Conlusion The balloon expanded.\n",
      "\n",
      "Step 1: IF you blow air into a balloon, THEN the balloon will fill with air.\n",
      "Assumption: Blowing air into a balloon introduces air into the balloon, increasing the internal air volume.\n",
      "\n",
      "Step 2: IF a balloon fills with air, THEN the balloon will expand.\n",
      "Assumption: Balloons are made of elastic material that stretches and expands as air volume inside increases.\n",
      "\n",
      "Step 3: Therefore, since you blew air into the balloon, the balloon filled with air, causing it to expand.\n",
      "\n",
      "Explanation 2:\n",
      "\n",
      "Hypothesis I pricked the baloon.\n",
      "Conclusion The balloon expanded.\n",
      "\n",
      "The provided hypothesis and conclusion seem to be contradictory, as pricking a balloon typically causes it to deflate rather than expand. However, let's explore a possible explanation by considering an alternative scenario where the conclusion could logically follow the hypothesis.\n",
      "\n",
      "Step 1: IF a balloon is pricked, THEN it typically deflates.\n",
      "Assumption: Pricking a balloon creates a hole, allowing the air inside to escape, leading to deflation.\n",
      "\n",
      "Step 2: IF a balloon is pricked and it expands, THEN there must be an external force or mechanism causing the expansion.\n",
      "Assumption: Balloons do not naturally expand when pricked unless influenced by an external factor.\n",
      "\n",
      "Step 3: IF a balloon is pricked and connected to a source of air or gas, THEN it can expand despite the puncture.\n",
      "Assumption: An external air or gas source can inflate a balloon even if it has a hole, as long as the inflow of air is greater than the outflow.\n",
      "\n",
      "Step 4: Therefore, if the balloon expanded after being pricked, it is likely due to an external air or gas source inflating it, overcoming the natural tendency to deflate.\n",
      "\n",
      "In this scenario, the conclusion that the balloon expanded after being pricked can be explained by the presence of an external air source that caused the expansion despite the puncture.\n"
     ]
    }
   ],
   "source": [
    "# Import the  critique models\n",
    "from critique import CoherenceCritique\n",
    "from critique import ParsimonyCritique\n",
    "from critique import UncertaintyCritique\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Import the generative GPT model\n",
    "from generation.generative_model import GPT\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Initialise the generative model (i.e. GPT-4o-mini)\n",
    "with open('config.yaml', 'r') as file:\n",
    "     config = yaml.safe_load(file)\n",
    "     api_key = config.get('gpt-4o', {}).get('api_key')\n",
    "\n",
    "llm = GPT('gpt-4o', api_key)\n",
    "\n",
    "\n",
    "# First hypothesis (change to premise)\n",
    "hypothesis_1 = \"I blew into the baloon.\"\n",
    "conclusion_1 =  \"The balloon expanded.\"\n",
    "\n",
    "# Second hypothesis (change to premise)\n",
    "hypothesis_2 = \"I pricked the baloon.\"\n",
    "conclusion_2 =  \"The balloon expanded.\"\n",
    "\n",
    "# Prompt the model to generate the explanation for the first hypothesis\n",
    "explanation_1 = llm.generate(\n",
    "             model_prompt_dir = 'ibe',\n",
    "             prompt_name = \"generate_explanation_prompt.txt\",\n",
    "             hypothesis = hypothesis_1,\n",
    "             conclusion = conclusion_1\n",
    "         )\n",
    "print(f\"\\nExplanation 1:\\n\\nHypothesis {hypothesis_1}\\nConlusion {conclusion_1}\\n\\n{explanation_1}\")\n",
    "\n",
    "\n",
    "# Prompt the model to generate the explanation for the first hypothesis\n",
    "explanation_2 = llm.generate(\n",
    "             model_prompt_dir = 'ibe',\n",
    "             prompt_name = \"generate_explanation_prompt.txt\",\n",
    "             hypothesis = hypothesis_2,\n",
    "             conclusion = conclusion_2\n",
    "         )\n",
    "print(f\"\\nExplanation 2:\\n\\nHypothesis {hypothesis_2}\\nConclusion {conclusion_2}\\n\\n{explanation_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate explanations via soft critique models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7387,
     "status": "ok",
     "timestamp": 1731635333117,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "TFIqeRU7vsO-",
    "outputId": "613aca8a-d0fa-4bec-c61c-ed3863f1d1b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Critique Evaluation\n",
      "\n",
      " ================ Coherence ================\n",
      "\n",
      "Explanation 1:  {'entailment': 0.7189022898674011, 'neutral': 0.27127864956855774, 'contradiction': 0.009819048456847668, 'score': 0.7090832414105535}\n",
      "Explanation 2:  {'entailment': 0.3860182464122772, 'neutral': 0.519865463177363, 'contradiction': 0.09411630034446716, 'score': 0.29190194606781006}\n",
      "Coherence comparision: Explanation 1: 0.7090832414105535 vs. Explanation 2: 0.29190194606781006\n",
      "Explanation 1 is therefore more coherent than Explanation 2.\n",
      "\n",
      "================ Parsimony ================\n",
      "\n",
      "Explanation 1:  {'score': 1}\n",
      "Explanation 2:  {'score': 12}\n",
      "\n",
      "Parsimony comparision: Explanation 1: 1 vs. Explanation 2: 12\n",
      "Explanation 1 is therefore more parsimonious than Explanation 2.\n",
      "\n",
      "================ Uncertainty ================\n",
      "\n",
      "Explanation 1:  {'score': 0.999363899230957}\n",
      "Explanation 2:  {'score': 1.5216406186421714}\n",
      "\n",
      "Uncertainty comparision: Explanation 1: 0.999363899230957 vs. Explanation 2: 1.5216406186421714\n",
      "Explanation 2 is therefore more uncertain than Explanation 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise the soft critique models\n",
    "coherence = CoherenceCritique()\n",
    "parsimony = ParsimonyCritique()\n",
    "uncertainty = UncertaintyCritique()\n",
    "\n",
    "print(\"Soft Critique Evaluation\")\n",
    "# Calculate and display soft critique scores\n",
    "\n",
    "# Coherence Metrics\n",
    "exp1_coherence = coherence.critique(explanation_1)\n",
    "exp2_coherence = coherence.critique(explanation_2)\n",
    "\n",
    "print(\"\\n ================ Coherence ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_coherence)\n",
    "print(\"Explanation 2: \", exp2_coherence)\n",
    "\n",
    "print(f\"Coherence comparision: Explanation 1: {exp1_coherence['score']} vs. Explanation 2: {exp2_coherence['score']}\")\n",
    "\n",
    "if exp1_coherence['score'] > exp2_coherence['score']:\n",
    "    print(\"Explanation 1 is therefore more coherent than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is the most coherente than Explanation 1.\")\n",
    "\n",
    "# Parsimony Metrics\n",
    "exp1_parsimony = parsimony.critique(hypothesis_1, conclusion_1, explanation_1)\n",
    "exp2_parsimony = parsimony.critique(hypothesis_2, conclusion_2, explanation_2)\n",
    "\n",
    "print(\"\\n================ Parsimony ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_parsimony)\n",
    "print(\"Explanation 2: \", exp2_parsimony)\n",
    "\n",
    "print(f\"\\nParsimony comparision: Explanation 1: {exp1_parsimony['score']} vs. Explanation 2: {exp2_parsimony['score']}\")\n",
    "\n",
    "if exp1_parsimony['score'] < exp2_parsimony['score']:\n",
    "    print(\"Explanation 1 is therefore more parsimonious than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is therefore more parsimonious than Explanation 1.\")\n",
    "\n",
    "# Uncertainty Metrics\n",
    "exp1_uncertainty = uncertainty.critique(explanation_1)\n",
    "exp2_uncertainty = uncertainty.critique(explanation_2)\n",
    "\n",
    "print(\"\\n================ Uncertainty ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_uncertainty)\n",
    "print(\"Explanation 2: \", exp2_uncertainty)\n",
    "\n",
    "print(f\"\\nUncertainty comparision: Explanation 1: {exp1_uncertainty['score']} vs. Explanation 2: {exp2_uncertainty['score']}\")\n",
    "\n",
    "if exp1_uncertainty['score'] > exp2_uncertainty['score']:\n",
    "    print(\"Explanation 1 is therefore more uncertain than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is therefore more uncertain than Explanation 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try with GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3122,
     "status": "ok",
     "timestamp": 1731635469816,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "UmW-kKd1iliL",
    "outputId": "5a2e31fd-ac53-4b01-b3c4-3722d101d03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanation 1:\n",
      "\n",
      "Hypothesis I blew into the baloon.\n",
      "Conlusion The balloon expanded.\n",
      "\n",
      "Step 1: IF air is blown into a balloon, THEN the pressure inside the balloon increases.\n",
      "Assumption: Blowing air into a balloon increases the air pressure inside it.\n",
      "\n",
      "Step 2: IF the pressure inside a balloon increases, THEN the balloon expands.\n",
      "Assumption: An increase in air pressure inside a balloon causes it to expand.\n",
      "\n",
      "Step 3: Therefore, since you blew air into the balloon, the pressure inside increased, leading to the expansion of the balloon.\n",
      "\n",
      "Explanation 2:\n",
      "\n",
      "Hypothesis I pricked the baloon.\n",
      "Conclusion The balloon expanded.\n",
      "\n",
      "Step 1: IF a balloon is pricked, THEN the air inside the balloon can escape.\n",
      "Assumption: When a balloon is pricked, it creates a hole through which the air inside can exit.\n",
      "\n",
      "Step 2: IF the air inside a balloon escapes, THEN the pressure inside the balloon decreases.\n",
      "Assumption: The air escaping from the balloon reduces the pressure inside it.\n",
      "\n",
      "Step 3: IF the pressure inside a balloon decreases, THEN the balloon material can expand.\n",
      "Assumption: A decrease in pressure can cause the balloon material to stretch and expand.\n",
      "\n",
      "Step 4: Therefore, since you pricked the balloon, causing the air to escape and the pressure to decrease, the balloon material expanded as a result.\n"
     ]
    }
   ],
   "source": [
    "# Import the  critique models\n",
    "from critique import CoherenceCritique\n",
    "from critique import ParsimonyCritique\n",
    "from critique import UncertaintyCritique\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Import the generative GPT model\n",
    "from generation.generative_model import GPT\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Initialise the generative model (i.e. GPT-4o)\n",
    "with open('config.yaml', 'r') as file:\n",
    "     config = yaml.safe_load(file)\n",
    "     api_key = config.get('gpt-3.5-turbo', {}).get('api_key')\n",
    "\n",
    "llm = GPT('gpt-3.5-turbo', api_key)\n",
    "\n",
    "\n",
    "# First hypothesis (change to premise)\n",
    "hypothesis_1 = \"I blew into the baloon.\"\n",
    "conclusion_1 =  \"The balloon expanded.\"\n",
    "\n",
    "# Second hypothesis (change to premise)\n",
    "hypothesis_2 = \"I pricked the baloon.\"\n",
    "conclusion_2 =  \"The balloon expanded.\"\n",
    "\n",
    "# Prompt the model to generate the explanation for the first hypothesis\n",
    "explanation_1 = llm.generate(\n",
    "             model_prompt_dir = 'ibe',\n",
    "             prompt_name = \"generate_explanation_prompt.txt\",\n",
    "             hypothesis = hypothesis_1,\n",
    "             conclusion = conclusion_1\n",
    "         )\n",
    "print(f\"\\nExplanation 1:\\n\\nHypothesis {hypothesis_1}\\nConlusion {conclusion_1}\\n\\n{explanation_1}\")\n",
    "\n",
    "\n",
    "# Prompt the model to generate the explanation for the first hypothesis\n",
    "explanation_2 = llm.generate(\n",
    "             model_prompt_dir = 'ibe',\n",
    "             prompt_name = \"generate_explanation_prompt.txt\",\n",
    "             hypothesis = hypothesis_2,\n",
    "             conclusion = conclusion_2\n",
    "         )\n",
    "print(f\"\\nExplanation 2:\\n\\nHypothesis {hypothesis_2}\\nConclusion {conclusion_2}\\n\\n{explanation_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate explanations via soft critique models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6764,
     "status": "ok",
     "timestamp": 1731635479444,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "j6Jb-1KgipEr",
    "outputId": "cff888ec-b8d8-4a29-df0a-b6a957ec3f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Critique Evaluation\n",
      "\n",
      " ================ Coherence ================\n",
      "\n",
      "Explanation 1:  {'entailment': 0.5016993284225464, 'neutral': 0.47439172863960266, 'contradiction': 0.023908991366624832, 'score': 0.47779033705592155}\n",
      "Explanation 2:  {'entailment': 0.48480911552906036, 'neutral': 0.4241005778312683, 'contradiction': 0.09109026193618774, 'score': 0.3937188535928726}\n",
      "Coherence comparision: Explanation 1: 0.47779033705592155 vs. Explanation 2: 0.3937188535928726\n",
      "Explanation 1 is therefore more coherent than Explanation 2.\n",
      "\n",
      "================ Parsimony ================\n",
      "\n",
      "Explanation 1:  {'score': 3}\n",
      "Explanation 2:  {'score': 4}\n",
      "\n",
      "Parsimony comparision: Explanation 1: 3 vs. Explanation 2: 4\n",
      "Explanation 1 is therefore more parsimonious than Explanation 2.\n",
      "\n",
      "================ Uncertainty ================\n",
      "\n",
      "Explanation 1:  {'score': 1.0386515855789185}\n",
      "Explanation 2:  {'score': 1.0947856903076172}\n",
      "\n",
      "Uncertainty comparision: Explanation 1: 1.0386515855789185 vs. Explanation 2: 1.0947856903076172\n",
      "Explanation 2 is therefore more uncertain than Explanation 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise the soft critique models\n",
    "coherence = CoherenceCritique()\n",
    "parsimony = ParsimonyCritique()\n",
    "uncertainty = UncertaintyCritique()\n",
    "\n",
    "print(\"Soft Critique Evaluation\")\n",
    "# Calculate and display soft critique scores\n",
    "\n",
    "# Coherence Metrics\n",
    "exp1_coherence = coherence.critique(explanation_1)\n",
    "exp2_coherence = coherence.critique(explanation_2)\n",
    "\n",
    "print(\"\\n ================ Coherence ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_coherence)\n",
    "print(\"Explanation 2: \", exp2_coherence)\n",
    "\n",
    "print(f\"Coherence comparision: Explanation 1: {exp1_coherence['score']} vs. Explanation 2: {exp2_coherence['score']}\")\n",
    "\n",
    "if exp1_coherence['score'] > exp2_coherence['score']:\n",
    "    print(\"Explanation 1 is therefore more coherent than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is the most coherente than Explanation 1.\")\n",
    "\n",
    "# Parsimony Metrics\n",
    "exp1_parsimony = parsimony.critique(hypothesis_1, conclusion_1, explanation_1)\n",
    "exp2_parsimony = parsimony.critique(hypothesis_2, conclusion_2, explanation_2)\n",
    "\n",
    "print(\"\\n================ Parsimony ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_parsimony)\n",
    "print(\"Explanation 2: \", exp2_parsimony)\n",
    "\n",
    "print(f\"\\nParsimony comparision: Explanation 1: {exp1_parsimony['score']} vs. Explanation 2: {exp2_parsimony['score']}\")\n",
    "\n",
    "if exp1_parsimony['score'] < exp2_parsimony['score']:\n",
    "    print(\"Explanation 1 is therefore more parsimonious than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is therefore more parsimonious than Explanation 1.\")\n",
    "\n",
    "# Uncertainty Metrics\n",
    "exp1_uncertainty = uncertainty.critique(explanation_1)\n",
    "exp2_uncertainty = uncertainty.critique(explanation_2)\n",
    "\n",
    "print(\"\\n================ Uncertainty ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_uncertainty)\n",
    "print(\"Explanation 2: \", exp2_uncertainty)\n",
    "\n",
    "print(f\"\\nUncertainty comparision: Explanation 1: {exp1_uncertainty['score']} vs. Explanation 2: {exp2_uncertainty['score']}\")\n",
    "\n",
    "if exp1_uncertainty['score'] > exp2_uncertainty['score']:\n",
    "    print(\"Explanation 1 is therefore more uncertain than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is therefore more uncertain than Explanation 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison \n",
    "\n",
    "The explanations generated by GPT-4-o for this example have a better \"separation\" than the ones generated by GPT-3.5-turbo.\n",
    "\n",
    "GPT-4o:\n",
    "\n",
    "- Coherence comparision:  Explanation 1: 0.7090832414105535 vs. Explanation 2: 0.29190194606781006\n",
    "- Parsimony comparision:  Explanation 1: 1 vs. Explanation 2: 12\n",
    "- Uncertainty comparision: Explanation 1: 0.999363899230957 vs. Explanation 2: 1.5216406186421714\n",
    "\n",
    "\n",
    "GPT-3.4-Turbo:\n",
    "\n",
    "- Coherence comparision: Explanation 1: 0.871249190531671 vs. Explanation 2: 0.06694453046657145\n",
    "- Parsimony comparision: Explanation 1: 3 vs. Explanation 2: 6\n",
    "- Uncertainty comparision: Explanation 1: 1.0230472882588706 vs. Explanation 2: 2.125597596168518"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1beM923HvLSUEf6eJ-bSOaqx9ICx7Fr_b",
     "timestamp": 1730906628931
    }
   ]
  },
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
